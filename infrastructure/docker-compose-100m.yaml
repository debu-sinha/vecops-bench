# VectorDB-Bench v2.0 - Docker Compose for 100M Scale
# Optimized for i4i.4xlarge (128GB RAM, NVMe storage)
#
# Key differences from v1.0:
# - Increased memory limits for 100M vectors
# - NVMe volume mounts for fast disk I/O
# - Health checks with longer timeouts
# - Resource limits to prevent OOM
#
# IMPORTANT: Run with --compatibility flag for memory limits to work:
#   docker-compose --compatibility -f docker-compose-100m.yaml up -d
# Or use Docker Compose v2 (docker compose) which supports deploy limits natively.

version: '3.8'

# Common settings
x-logging: &default-logging
  driver: json-file
  options:
    max-size: "100m"
    max-file: "5"

x-restart: &default-restart
  restart: unless-stopped

services:
  # ===========================================================================
  # Milvus (Distributed Vector DB)
  # ===========================================================================
  etcd:
    image: quay.io/coreos/etcd:v3.5.11
    container_name: milvus-etcd
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - /data/etcd:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 20s
      retries: 3
    <<: *default-logging
    <<: *default-restart
    mem_limit: 2g
    memswap_limit: 2g
    deploy:
      resources:
        limits:
          memory: 2G

  minio:
    image: minio/minio:RELEASE.2024-01-01T16-36-33Z
    container_name: milvus-minio
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - /data/minio:/data
    command: minio server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    <<: *default-logging
    <<: *default-restart
    mem_limit: 4g
    memswap_limit: 4g
    deploy:
      resources:
        limits:
          memory: 4G

  milvus:
    image: milvusdb/milvus:v2.3.4
    container_name: milvus
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
    volumes:
      - /data/milvus:/var/lib/milvus
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      etcd:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 120s
    <<: *default-logging
    <<: *default-restart
    mem_limit: 32g
    memswap_limit: 32g
    deploy:
      resources:
        limits:
          memory: 32G  # Increased for 100M vectors

  # ===========================================================================
  # Qdrant (Native Vector DB)
  # ===========================================================================
  qdrant:
    image: qdrant/qdrant:v1.7.4
    container_name: qdrant
    volumes:
      - /data/qdrant:/qdrant/storage
    ports:
      - "6333:6333"
      - "6334:6334"
    environment:
      - QDRANT__STORAGE__STORAGE_PATH=/qdrant/storage
      - QDRANT__SERVICE__GRPC_PORT=6334
      # Enable on-disk storage for 100M scale
      - QDRANT__STORAGE__ON_DISK_PAYLOAD=true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/"]
      interval: 30s
      timeout: 10s
      retries: 5
    <<: *default-logging
    <<: *default-restart
    mem_limit: 32g
    memswap_limit: 32g
    deploy:
      resources:
        limits:
          memory: 32G

  # ===========================================================================
  # pgvector (PostgreSQL Extension)
  # ===========================================================================
  pgvector:
    image: pgvector/pgvector:pg16
    container_name: pgvector
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: vectordb_bench
      # Tune for large datasets
      POSTGRES_SHARED_BUFFERS: 8GB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 24GB
      POSTGRES_WORK_MEM: 1GB
      POSTGRES_MAINTENANCE_WORK_MEM: 4GB
    volumes:
      - /data/pgvector:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    command: >
      postgres
      -c shared_buffers=8GB
      -c effective_cache_size=24GB
      -c work_mem=1GB
      -c maintenance_work_mem=4GB
      -c max_connections=100
      -c checkpoint_completion_target=0.9
      -c wal_buffers=64MB
      -c random_page_cost=1.1
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    <<: *default-logging
    <<: *default-restart
    mem_limit: 32g
    memswap_limit: 32g
    deploy:
      resources:
        limits:
          memory: 32G

  # ===========================================================================
  # Weaviate (GraphQL Vector DB)
  # ===========================================================================
  weaviate:
    image: semitechnologies/weaviate:1.23.7
    container_name: weaviate
    volumes:
      - /data/weaviate:/var/lib/weaviate
    ports:
      - "8080:8080"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
      ENABLE_MODULES: ''
      CLUSTER_HOSTNAME: 'node1'
      # Memory settings for 100M
      LIMIT_RESOURCES: 'false'
      GOMAXPROCS: '16'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/.well-known/ready"]
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 60s
    <<: *default-logging
    <<: *default-restart
    mem_limit: 32g
    memswap_limit: 32g
    deploy:
      resources:
        limits:
          memory: 32G

  # ===========================================================================
  # ChromaDB (Embedded-first)
  # ===========================================================================
  chroma:
    image: chromadb/chroma:0.4.22
    container_name: chroma
    volumes:
      - /data/chroma:/chroma/chroma
    ports:
      - "8000:8000"
    environment:
      IS_PERSISTENT: 'TRUE'
      PERSIST_DIRECTORY: '/chroma/chroma'
      ANONYMIZED_TELEMETRY: 'FALSE'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 5
    <<: *default-logging
    <<: *default-restart
    mem_limit: 16g
    memswap_limit: 16g
    deploy:
      resources:
        limits:
          memory: 16G  # Chroma uses less memory

  # ===========================================================================
  # Elasticsearch (Enterprise Incumbent)
  # ===========================================================================
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
      - ES_JAVA_OPTS=-Xms16g -Xmx16g
      - cluster.routing.allocation.disk.threshold_enabled=false
    volumes:
      - /data/elasticsearch:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9200/_cluster/health"]
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 120s
    <<: *default-logging
    <<: *default-restart
    mem_limit: 24g
    memswap_limit: 24g
    deploy:
      resources:
        limits:
          memory: 24G

# ===========================================================================
# Networks
# ===========================================================================
networks:
  default:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16
