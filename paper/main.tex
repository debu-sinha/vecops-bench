% VectorDB-Bench: A Production-Oriented Evaluation Framework for Vector Databases
% Target: MLSys 2026

\documentclass{article}

% MLSys 2026 style (use appropriate style file when available)
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}

% Custom commands
\newcommand{\system}{VectorDB-Bench}
\newcommand{\todo}[1]{\textcolor{red}{[TODO: #1]}}

\title{\system{}: A Production-Oriented Evaluation Framework for Vector Databases}

\author{
  Debu Sinha \\
  Independent Researcher \\
  \texttt{debu.sinha@example.com}
}

\begin{document}

\maketitle

\begin{abstract}
Vector databases have become critical infrastructure for AI applications, yet existing benchmarks focus primarily on recall and latency metrics that fail to predict production performance. We introduce \system{}, a comprehensive evaluation framework that measures production-relevant dimensions including hybrid search accuracy, metadata filtering overhead, cold-start latency, index build costs, and operational complexity. We evaluate six popular vector databases---Pinecone, Milvus, Qdrant, Weaviate, pgvector, and Chroma---across multiple datasets and workload patterns. Our results reveal significant gaps between academic benchmarks and production behavior: databases with similar recall@10 scores can differ by 5$\times$ in cold-start latency and 3$\times$ in filtered search overhead. We release \system{} as an open-source toolkit to enable reproducible, production-aware vector database evaluation.
\end{abstract}

\section{Introduction}
\label{sec:intro}

The rise of embedding-based AI applications---from semantic search to retrieval-augmented generation (RAG)---has driven explosive growth in vector database adoption. Organizations must choose among dozens of options including purpose-built systems (Pinecone, Milvus, Qdrant, Weaviate), database extensions (pgvector), and lightweight libraries (Chroma, FAISS).

Existing benchmarks like ann-benchmarks.com focus on approximate nearest neighbor (ANN) search quality, measuring recall@k against brute-force search and queries per second (QPS). While valuable, these metrics capture only a fraction of production concerns:

\begin{itemize}
    \item \textbf{Cold-start latency}: How quickly can the system serve its first query after deployment or restart?
    \item \textbf{Filtered search}: What is the overhead of combining vector similarity with metadata filters?
    \item \textbf{Hybrid search}: How effectively does the system combine dense and sparse retrieval?
    \item \textbf{Operational complexity}: How difficult is deployment, monitoring, and maintenance?
    \item \textbf{Cost efficiency}: What is the cost per query at production scale?
\end{itemize}

We present \system{}, a production-oriented evaluation framework that addresses these gaps. Our contributions are:

\begin{enumerate}
    \item A comprehensive benchmark suite measuring six production-relevant dimensions beyond recall/latency
    \item Evaluation of six popular vector databases across multiple datasets and workload patterns
    \item Analysis revealing significant discrepancies between academic metrics and production performance
    \item An open-source toolkit enabling reproducible vector database evaluation
\end{enumerate}

\section{Related Work}
\label{sec:related}

\textbf{ANN Benchmarks.} The ann-benchmarks project~\cite{annbenchmarks} established standard methodology for comparing ANN algorithms on recall and QPS. However, it evaluates algorithms rather than production systems and ignores deployment concerns.

\textbf{Vector Database Comparisons.} Industry comparisons from Pinecone, Weaviate, and others typically benchmark against competitors using favorable configurations. Academic evaluations remain sparse.

\textbf{Database Benchmarking.} Traditional database benchmarks (TPC-C, YCSB) provide comprehensive evaluation frameworks but don't address vector search specifics.

\textbf{RAG Evaluation.} Recent work on RAG benchmarks (CRAG, RAGBench) evaluates end-to-end retrieval-augmented generation but treats the vector database as a black box.

\section{Benchmark Design}
\label{sec:design}

\subsection{Evaluation Dimensions}

\system{} evaluates vector databases across six dimensions:

\textbf{1. Standard Metrics (Baseline).} Recall@k, precision@k, NDCG@k, and latency percentiles (p50, p95, p99) provide baseline comparison with existing benchmarks.

\textbf{2. Cold-Start Latency.} We measure time from connection establishment to first successful query, simulating container restarts and autoscaling events common in production.

\textbf{3. Filtered Search Performance.} Production queries often combine vector similarity with metadata filters (e.g., "find similar documents from 2024"). We measure the latency overhead of increasingly complex filter predicates.

\textbf{4. Hybrid Search Accuracy.} For databases supporting hybrid (dense + sparse) retrieval, we evaluate the effectiveness of combining embedding similarity with keyword matching.

\textbf{5. Index Build Characteristics.} We measure index construction time, memory consumption during build, and throughput (vectors/second).

\textbf{6. Workload Robustness.} Beyond uniform random queries, we evaluate performance under realistic workload patterns: bursty traffic (Poisson arrivals), skewed distributions (Zipfian), and filtered queries.

\subsection{Datasets}

We use standard retrieval benchmarks from BEIR~\cite{beir}:
\begin{itemize}
    \item \textbf{SciFact}: 5K scientific claims, 300 queries
    \item \textbf{NFCorpus}: 3.6K medical documents, 323 queries
    \item \textbf{MS MARCO}: 8.8M passages (sampled to 100K), 6.9K queries
\end{itemize}

All documents are embedded using \texttt{all-mpnet-base-v2} (768 dimensions) from Sentence Transformers.

\subsection{Databases Evaluated}

We evaluate six vector databases representing different architectural approaches:

\begin{itemize}
    \item \textbf{Pinecone}: Managed cloud service (serverless tier)
    \item \textbf{Milvus}: Open-source, distributed architecture
    \item \textbf{Qdrant}: Open-source, Rust-based
    \item \textbf{Weaviate}: Open-source, GraphQL-native
    \item \textbf{pgvector}: PostgreSQL extension
    \item \textbf{Chroma}: Lightweight, embedded-first
\end{itemize}

\section{Experimental Setup}
\label{sec:setup}

\subsection{Hardware Environment}

All self-hosted databases are evaluated on a standardized cloud instance to ensure reproducibility and fair comparison. Table~\ref{tab:hardware} details our experimental environment.

\begin{table}[h]
\centering
\caption{Experimental Hardware Environment}
\label{tab:hardware}
\begin{tabular}{ll}
\toprule
Component & Specification \\
\midrule
Instance Type & AWS EC2 c5.2xlarge \\
CPU & Intel Xeon Platinum 8275CL @ 3.0GHz \\
vCPUs & 8 \\
Memory & 16 GB \\
Storage & 100 GB gp3 SSD (3000 IOPS) \\
Network & Up to 10 Gbps \\
Region & us-east-1 \\
OS & Ubuntu 22.04 LTS \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Software Versions}

For reproducibility, all software versions are pinned. Docker images use specific version tags rather than \texttt{:latest}. Table~\ref{tab:versions} lists the evaluated database versions.

\begin{table}[h]
\centering
\caption{Database Versions}
\label{tab:versions}
\begin{tabular}{lll}
\toprule
Database & Version & Deployment \\
\midrule
Qdrant & v1.7.4 & Docker \\
Milvus & v2.3.4 & Docker (standalone) \\
Weaviate & 1.23.0 & Docker \\
pgvector & 0.6.0 (PG 16) & Docker \\
Chroma & 0.4.22 & Docker \\
Pinecone & Serverless & Managed Cloud \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Note on Pinecone.} As a managed service, Pinecone is evaluated separately. Latency measurements include network round-trip time ($\sim$5-20ms) and are not directly comparable to self-hosted deployments. We include Pinecone primarily for cost-efficiency and recall comparisons.

\subsection{Index Configuration}

All databases use HNSW indexing where available with consistent parameters:
\begin{itemize}
    \item HNSW M (max connections): 16
    \item HNSW efConstruction: 128
    \item HNSW efSearch: 64
    \item Distance metric: Cosine similarity
\end{itemize}

\subsection{Methodology}

\textbf{Statistical Rigor.} Each benchmark configuration runs 5 times. We report means and standard deviations. Outliers beyond 3$\sigma$ are noted but included unless hardware anomalies are detected.

\textbf{Cache Management.} Between runs, we clear filesystem caches (\texttt{sync \&\& echo 3 > /proc/sys/vm/drop\_caches}) and restart database containers to eliminate warm-cache effects.

\textbf{Query Sampling.} Queries are sampled uniformly from the test set unless otherwise specified for workload pattern experiments.

\section{Results}
\label{sec:results}

\subsection{Standard Metrics}

Table~\ref{tab:standard} shows baseline recall and latency results on SciFact.

\begin{table}[h]
\centering
\caption{Standard benchmark results on SciFact (5K documents)}
\label{tab:standard}
\begin{tabular}{lcccc}
\toprule
Database & Recall@10 & Latency p50 & Latency p95 & QPS \\
\midrule
Pinecone & \todo{X.XXX} & \todo{X.XX} & \todo{X.XX} & \todo{XXX} \\
Milvus & \todo{X.XXX} & \todo{X.XX} & \todo{X.XX} & \todo{XXX} \\
Qdrant & \todo{X.XXX} & \todo{X.XX} & \todo{X.XX} & \todo{XXX} \\
Weaviate & \todo{X.XXX} & \todo{X.XX} & \todo{X.XX} & \todo{XXX} \\
pgvector & \todo{X.XXX} & \todo{X.XX} & \todo{X.XX} & \todo{XXX} \\
Chroma & \todo{X.XXX} & \todo{X.XX} & \todo{X.XX} & \todo{XXX} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Cold-Start Latency}

Figure~\ref{fig:coldstart} shows cold-start latency across databases. \todo{Add figure and analysis.}

\subsection{Filtered Search Overhead}

\todo{Add filtered search results and analysis.}

\subsection{Index Build Performance}

\todo{Add index build results and analysis.}

\subsection{Workload Robustness}

\todo{Add workload pattern results.}

\section{Discussion}
\label{sec:discussion}

\textbf{Finding 1: Recall parity hides production differences.} Databases achieving similar recall@10 can differ dramatically on production metrics.

\textbf{Finding 2: Cold-start varies by orders of magnitude.} \todo{Elaborate.}

\textbf{Finding 3: Filtered search is not free.} \todo{Elaborate.}

\textbf{Limitations.} Our evaluation uses default configurations; production deployments may tune parameters. We evaluate single-node setups; distributed behavior may differ.

\section{Conclusion}
\label{sec:conclusion}

We presented \system{}, a production-oriented evaluation framework for vector databases. Our benchmark reveals that standard recall/latency metrics fail to capture important production dimensions. We release \system{} as open-source software to enable reproducible, comprehensive vector database evaluation.

\section*{Reproducibility}

Code, data, and instructions are available at: \url{https://github.com/debu-sinha/vectordb-bench}

\bibliographystyle{plain}
\bibliography{references}

\appendix

\section{Extended Results}
\label{app:extended}

\todo{Add extended results tables.}

\section{Configuration Details}
\label{app:config}

\todo{Add detailed configuration for each database.}

\end{document}
