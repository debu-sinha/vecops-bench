% VecOps-Bench: Measuring the Hidden Cost of Data Churn in Vector Databases
% Target: VLDB 2026
% Author: Debu Sinha

\documentclass[sigconf]{acmart}

% ACM metadata
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}
\settopmatter{printacmref=false}
\acmConference[VLDB 2026]{Proceedings of the VLDB Endowment}{2026}{TBD}
\acmYear{2026}
\copyrightyear{2026}

% Packages
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{balance}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}

% Commands
\newcommand{\vecops}{VecOps-Bench}
\newcommand{\recall}{Recall@10}

\title{VecOps-Bench: Measuring the Hidden Cost of Data Churn in Production Vector Databases}

\author{Debu Sinha}
\email{debusinha2009@gmail.com}
\affiliation{%
  \institution{Independent Researcher}
  \country{USA}
}

\begin{abstract}
Vector databases have become critical infrastructure for AI applications, yet existing benchmarks focus exclusively on initial load performance---the ``Day 1'' problem. We present \vecops{}, the first systematic benchmark measuring how HNSW-based vector databases perform under continuous data churn---the ``Day 2'' problem that production systems face. Our key finding: \textbf{all tested vector databases experience significant recall degradation under data churn}, with degradation ranging from 3.4\% to 9.7\% after just 10\% corpus turnover. Contrary to expectations, pgvector (disk-backed with PostgreSQL's VACUUM) shows the \emph{highest} degradation (9.7\%), while Weaviate shows the lowest (3.4\%). We also document dramatic differences in churn operation speed: Milvus completes a 100K delete+insert cycle in 36 seconds, while pgvector requires 23 minutes---a 38$\times$ difference. Our results are validated on two independent datasets (Cohere Wikipedia 9.99M and SIFT1M) and reveal a previously undocumented phenomenon with significant implications for production deployments. We release \vecops{} as open source to enable reproducible evaluation of vector database temporal stability.
\end{abstract}

\keywords{vector databases, benchmarking, HNSW, temporal drift, data churn, production systems}

\begin{document}

\maketitle

%=============================================================================
% SECTION 1: INTRODUCTION
%=============================================================================
\section{Introduction}

The rise of large language models and embedding-based AI has driven explosive growth in vector database adoption. Organizations must choose among dozens of options to power semantic search, retrieval-augmented generation (RAG), and recommendation systems. Existing benchmarks like ann-benchmarks~\cite{annbenchmarks} and VectorDBBench help evaluate initial performance---query latency, recall, and ingestion speed on freshly loaded data.

However, production vector databases face a challenge these benchmarks ignore: \textbf{continuous data churn}. Real-world corpora evolve as documents are added, updated, and deleted. News articles expire. Product catalogs change. User-generated content gets moderated. This ongoing churn affects the underlying HNSW (Hierarchical Navigable Small World) indexes that power most vector databases.

\textbf{The Day 2 Problem.} While benchmarks measure ``Day 1'' performance (fresh index, optimal structure), production systems must maintain quality on ``Day 2'' and beyond---after weeks or months of data modifications have potentially degraded index quality.

\textbf{Our Contribution.} We present the first systematic measurement of recall degradation under data churn across production vector databases. Our key findings:

\begin{enumerate}
    \item \textbf{Universal degradation}: All four tested HNSW-based databases show statistically significant recall degradation under 10\% data churn (Table~\ref{tab:main_results}).
    \item \textbf{Unexpected ordering}: pgvector (disk-backed, with VACUUM) degrades \emph{most} (9.7\%), not least. Weaviate degrades least (3.4\%).
    \item \textbf{Operational speed varies 38$\times$}: Milvus handles churn in 36 seconds per cycle; pgvector requires 23 minutes.
    \item \textbf{Cross-dataset validation}: Results are consistent across Cohere Wikipedia (9.99M$\times$768) and SIFT1M (1M$\times$128).
\end{enumerate}

%=============================================================================
% SECTION 2: BACKGROUND AND MOTIVATION
%=============================================================================
\section{Background and Motivation}

\subsection{HNSW Index Structure}

The Hierarchical Navigable Small World (HNSW) algorithm~\cite{hnsw} is the dominant index structure for approximate nearest neighbor search in production systems. HNSW builds a multi-layer graph where:
\begin{itemize}
    \item Each vector is a node with connections to its approximate neighbors
    \item Higher layers provide coarse navigation; lower layers enable fine-grained search
    \item Parameter $M$ controls the number of connections per node
    \item Parameters $ef\_construction$ and $ef\_search$ control build-time and query-time quality
\end{itemize}

\textbf{The deletion problem.} When a vector is deleted from an HNSW index, its node is removed but the graph structure retains ``holes''---neighbors that previously connected through the deleted node must now find longer paths. This structural degradation accumulates with repeated deletions, potentially reducing recall.

\subsection{Existing Benchmarks}

\textbf{ann-benchmarks}~\cite{annbenchmarks} evaluates ANN algorithms on static datasets, measuring recall vs. queries-per-second. It does not measure post-churn performance.

\textbf{VectorDBBench} (Zilliz) benchmarks vector databases on ingestion and query performance but focuses on initial load, not sustained operations.

\textbf{BEIR}~\cite{beir} provides retrieval benchmarks but evaluates end-to-end retrieval quality, not index-level phenomena.

\subsection{Research Questions}

We address three research questions:
\begin{enumerate}
    \item[\textbf{RQ1}] Do HNSW indexes degrade under data churn?
    \item[\textbf{RQ2}] How does degradation vary across database implementations?
    \item[\textbf{RQ3}] What operational factors (speed, failure modes) affect churn handling?
\end{enumerate}

%=============================================================================
% SECTION 3: METHODOLOGY
%=============================================================================
\section{Methodology}

\subsection{Databases Under Test}

We evaluate four production vector databases, all using HNSW indexing:

\begin{table}[h]
\centering
\caption{Databases Evaluated}
\label{tab:databases}
\begin{tabular}{llll}
\toprule
\textbf{Database} & \textbf{Version} & \textbf{Architecture} & \textbf{Storage} \\
\midrule
Milvus & 2.3.4 & Distributed & In-memory \\
pgvector & 0.8.0 & PostgreSQL ext. & Disk-backed \\
Weaviate & 1.27.0 & GraphQL-native & Hybrid \\
Chroma & 0.6.3 & Embedded-first & In-memory \\
\bottomrule
\end{tabular}
\end{table}

We also tested Qdrant (1.16.0), but its filter-based delete operations timed out after 300 seconds even for small batches, making churn testing infeasible. This timeout behavior is itself a significant finding for operational complexity.

\subsection{Standardized HNSW Parameters}

To ensure fair comparison, all databases use identical HNSW parameters:

\begin{table}[h]
\centering
\caption{HNSW Configuration}
\label{tab:hnsw}
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Rationale} \\
\midrule
M & 16 & Industry standard \\
ef\_construction & 128 & Balanced build quality \\
ef\_search & 100 & Balanced recall/latency \\
Metric & Cosine & Standard for embeddings \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Datasets}

\textbf{Primary dataset: Cohere Wikipedia} \\
9,990,000 vectors $\times$ 768 dimensions from Cohere/Wikipedia-22-12-en-embeddings. Real text embeddings representing production workloads.

\textbf{Validation dataset: SIFT1M} \\
1,000,000 vectors $\times$ 128 dimensions. Standard ann-benchmarks dataset enabling comparison with prior work.

\subsection{Held-Out Query Methodology}

A critical methodological innovation: we use \emph{held-out queries} to prevent the common error of queries finding themselves in the index.

\begin{itemize}
    \item \textbf{Corpus}: 9,990,000 vectors indexed
    \item \textbf{Queries}: 10,000 vectors held out (never indexed)
    \item \textbf{Ground truth}: FAISS brute-force exact search on full 10M corpus
\end{itemize}

Recall is computed as:
\begin{equation}
\text{Recall@}k = \frac{|\text{returned} \cap \text{ground\_truth}|}{k}
\end{equation}

\subsection{Churn Protocol}

Our churn test simulates 10\% corpus turnover:

\begin{algorithm}
\caption{Churn Test Protocol}
\begin{algorithmic}[1]
\STATE Load 9,990,000 vectors into database
\STATE Measure baseline Recall@10 (cycle 0)
\FOR{cycle $= 1$ to $10$}
    \STATE Delete 100,000 random vectors
    \STATE Insert 100,000 new random vectors
    \STATE Measure Recall@10 using held-out queries
    \STATE Record latency (p50, p99)
    \STATE Record delete and insert times
\ENDFOR
\STATE Compute total degradation: $\frac{\text{initial} - \text{final}}{\text{initial}}$
\end{algorithmic}
\end{algorithm}

After 10 cycles, 1,000,000 vectors have been churned (10\% of corpus).

\subsection{Infrastructure}

All experiments run on AWS EC2 instances:
\begin{itemize}
    \item \textbf{Type}: Memory-optimized (123GB RAM, 16 vCPU)
    \item \textbf{Storage}: NVMe SSD
    \item \textbf{Isolation}: One database per VM to avoid memory contention
\end{itemize}

%=============================================================================
% SECTION 4: RESULTS
%=============================================================================
\section{Results}

\subsection{Main Result: Temporal Drift Under Churn}

Table~\ref{tab:main_results} presents our primary finding: all tested databases show statistically significant recall degradation under 10\% data churn.

\begin{table}[h]
\centering
\caption{Temporal Drift Results (10 Churn Cycles)}
\label{tab:main_results}
\begin{tabular}{lccc}
\toprule
\textbf{Database} & \textbf{Initial Recall@10} & \textbf{Final Recall@10} & \textbf{Degradation} \\
\midrule
pgvector & 83.13\% & 75.06\% & \textbf{9.71\%} \\
Milvus & 97.84\% & 89.28\% & \textbf{8.75\%} \\
Chroma & 88.99\% & 81.61\% & \textbf{8.29\%} \\
Weaviate & 81.80\% & 79.06\% & \textbf{3.35\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key finding}: pgvector shows the \emph{highest} degradation (9.71\%) despite being disk-backed with access to PostgreSQL's VACUUM. This contradicts the naive expectation that disk-backed systems with maintenance operations would be more stable.

\subsection{Cycle-by-Cycle Degradation}

Figure~\ref{fig:degradation} shows recall degradation over 10 churn cycles. All databases show monotonic decline, with Milvus starting highest (97.84\%) and pgvector showing steepest decline.

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{figures/fig1_recall_degradation.pdf}
\caption{Recall@10 degradation over 10 churn cycles. All databases show monotonic decline. Milvus maintains highest absolute recall but experiences similar percentage degradation to others. Annotations show total degradation after 10\% corpus turnover.}
\label{fig:degradation}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\columnwidth]{figures/fig2_degradation_bar.pdf}
\caption{Total recall degradation after 10\% corpus churn. pgvector shows highest degradation (9.71\%) while Weaviate shows best stability (3.35\%).}
\label{fig:degradation_bar}
\end{figure}

\subsection{Churn Operation Speed}

Operational speed varies dramatically---a 38$\times$ difference between fastest and slowest:

\begin{table}[h]
\centering
\caption{Churn Operation Speed (per 100K vectors)}
\label{tab:speed}
\begin{tabular}{lccc}
\toprule
\textbf{Database} & \textbf{Delete (s)} & \textbf{Insert (s)} & \textbf{Total Cycle (s)} \\
\midrule
Milvus & 22.9 & 14.3 & \textbf{37.2} \\
Weaviate & 94.9 & 94.9 & 189.8 \\
pgvector & 87.5 & 1,307.8 & \textbf{1,395.3} \\
Chroma & 671.5 & 781.8 & 1,453.3 \\
Qdrant & TIMEOUT & --- & \textbf{FAILED} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key findings}:
\begin{itemize}
    \item \textbf{Milvus} is 38$\times$ faster than pgvector at churn operations
    \item \textbf{pgvector} insert is extremely slow (1,307s = 22 minutes per 100K)
    \item \textbf{Qdrant} delete operations timeout, making it unsuitable for high-churn workloads
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{figures/fig3_churn_speed.pdf}
\caption{Churn operation speed per 100K vectors (log scale). Milvus completes a full churn cycle in 37s while pgvector requires 23 minutes---a 38$\times$ difference.}
\label{fig:churn_speed}
\end{figure}

\subsection{SIFT1M Cross-Validation}

To validate our findings generalize beyond Cohere embeddings, we run baseline tests on SIFT1M:

\begin{table}[h]
\centering
\caption{SIFT1M Validation Results}
\label{tab:sift1m}
\begin{tabular}{lcccc}
\toprule
\textbf{Database} & \textbf{Recall@1} & \textbf{Recall@10} & \textbf{Recall@100} & \textbf{P50 (ms)} \\
\midrule
Milvus & 98.38\% & 98.75\% & 97.14\% & 7.47 \\
Chroma & 97.45\% & 97.41\% & 91.73\% & 1.23 \\
pgvector & 95.86\% & 94.41\% & 40.64\% & 3.89 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Notable finding}: pgvector's Recall@100 is anomalously low (40.64\%) on both datasets, suggesting a systematic issue with its HNSW implementation or parameter tuning at higher $k$ values.

\begin{figure}[ht]
\centering
\includegraphics[width=\columnwidth]{figures/fig4_sift1m_recall.pdf}
\caption{SIFT1M validation results. pgvector's anomalously low Recall@100 (40.64\%) is notable---the same pattern appears on Cohere data.}
\label{fig:sift1m}
\end{figure}

\subsection{Qdrant Baseline (Churn Blocked)}

While Qdrant churn testing failed due to delete timeouts, baseline results are valid:

\begin{itemize}
    \item Recall@10: 96.74\%
    \item Recall@100: 95.46\%
    \item P50 latency: 8.14ms
\end{itemize}

The delete timeout (300s even for 100 vectors) is itself a significant finding for operational complexity---Qdrant's filter-based delete is not suitable for production churn workloads.

%=============================================================================
% SECTION 5: ANALYSIS
%=============================================================================
\section{Analysis}

\subsection{Why Does pgvector Degrade Most?}

The counterintuitive finding that pgvector (disk-backed, with VACUUM) shows highest degradation warrants analysis:

\begin{enumerate}
    \item \textbf{VACUUM timing}: PostgreSQL's autovacuum runs asynchronously and may not reclaim deleted vectors before new queries execute
    \item \textbf{Index structure}: pgvector's HNSW implementation may handle deletions differently than purpose-built vector databases
    \item \textbf{WAL overhead}: Write-ahead logging for ACID compliance may affect index maintenance timing
\end{enumerate}

\subsection{Why Does Weaviate Degrade Least?}

Weaviate's 3.35\% degradation (lowest) suggests more aggressive index maintenance:

\begin{enumerate}
    \item \textbf{Immediate compaction}: Weaviate may perform inline index repair during operations
    \item \textbf{Different deletion strategy}: Soft deletes with deferred cleanup may preserve graph structure longer
    \item \textbf{Lower baseline recall}: Starting at 81.80\% leaves less room for absolute degradation
\end{enumerate}

\subsection{Implications for Production}

\textbf{For high-churn workloads} (news, social media, e-commerce catalogs):
\begin{itemize}
    \item Milvus offers best combination of high recall and fast churn handling
    \item Schedule regular index rebuilds or compaction operations
    \item Monitor recall over time, not just at deployment
\end{itemize}

\textbf{For low-churn workloads} (archives, static documents):
\begin{itemize}
    \item Initial recall is primary concern; degradation is minimal
    \item pgvector's PostgreSQL integration may outweigh its churn limitations
\end{itemize}

\textbf{For workloads requiring deletes}:
\begin{itemize}
    \item Avoid Qdrant for frequent deletes (timeout issues)
    \item Budget 23+ minutes per 100K deletes if using pgvector
\end{itemize}

%=============================================================================
% SECTION 6: RELATED WORK
%=============================================================================
\section{Related Work}

\textbf{ANN Algorithm Benchmarks.} ann-benchmarks~\cite{annbenchmarks} establishes methodology for comparing ANN algorithms on recall vs. QPS. Our work extends this to measure temporal stability.

\textbf{Vector Database Surveys.} Recent surveys~\cite{vdbsurvey2025} document the proliferation of vector databases but do not measure churn behavior.

\textbf{Index Maintenance.} Research on B-tree and LSM-tree maintenance~\cite{lsmtree} informs our understanding of why indexes degrade, but HNSW-specific churn analysis is novel.

\textbf{HNSW Theory.} Malkov and Yashunin~\cite{hnsw} describe the algorithm but do not analyze deletion behavior at scale.

%=============================================================================
% SECTION 7: LIMITATIONS
%=============================================================================
\section{Limitations}

\begin{itemize}
    \item \textbf{Single-node only}: Distributed deployments may behave differently
    \item \textbf{No compaction experiment}: We did not test post-churn compaction recovery
    \item \textbf{Fixed parameters}: Expert tuning could improve individual results
    \item \textbf{Random churn}: Real workloads may have locality patterns
    \item \textbf{Qdrant excluded}: Delete timeout prevented churn measurement
\end{itemize}

%=============================================================================
% SECTION 8: CONCLUSION
%=============================================================================
\section{Conclusion}

We present the first systematic measurement of recall degradation under data churn in production vector databases. Our findings:

\begin{enumerate}
    \item \textbf{All HNSW databases degrade}: 3.4\%--9.7\% after 10\% churn
    \item \textbf{Disk-backed is not safer}: pgvector degrades most despite VACUUM
    \item \textbf{Speed varies 38$\times$}: Milvus handles churn in 37s; pgvector needs 23 minutes
    \item \textbf{Qdrant delete times out}: Not suitable for high-churn workloads
\end{enumerate}

For practitioners, we recommend: (1) monitor recall over time, not just at deployment; (2) choose Milvus for high-churn workloads; (3) schedule periodic index rebuilds; (4) test delete operations before production deployment.

We release \vecops{} at \url{https://github.com/debu-sinha/vecops-bench} to enable reproducible evaluation of vector database temporal stability.

%=============================================================================
% REFERENCES
%=============================================================================
\bibliographystyle{ACM-Reference-Format}
\begin{thebibliography}{99}

% ===== Core ANN and HNSW =====
\bibitem{hnsw}
Y.~A. Malkov and D.~A. Yashunin.
\newblock Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs.
\newblock {\em IEEE TPAMI}, 42(4):824--836, 2020.

\bibitem{nsw}
Y.~Malkov, A.~Ponomarenko, A.~Logvinov, and V.~Krylov.
\newblock Approximate nearest neighbor algorithm based on navigable small world graphs.
\newblock {\em Information Systems}, 45:61--68, 2014.

\bibitem{productquantization}
H.~J{\'e}gou, M.~Douze, and C.~Schmid.
\newblock Product quantization for nearest neighbor search.
\newblock {\em IEEE TPAMI}, 33(1):117--128, 2011.

\bibitem{ivfpq}
H.~J{\'e}gou, R.~Tavenard, M.~Douze, and L.~Amsaleg.
\newblock Searching in one billion vectors: Re-rank with source coding.
\newblock In {\em IEEE ICASSP}, pages 861--864, 2011.

\bibitem{faiss}
J.~Johnson, M.~Douze, and H.~J{\'e}gou.
\newblock Billion-scale similarity search with GPUs.
\newblock {\em IEEE Transactions on Big Data}, 7(3):535--547, 2021.

\bibitem{scann}
R.~Guo, P.~Sun, E.~Lindgren, Q.~Geng, D.~Simcha, F.~Chern, and S.~Kumar.
\newblock Accelerating large-scale inference with anisotropic vector quantization.
\newblock In {\em ICML}, pages 3887--3896, 2020.

\bibitem{diskann}
S.~Jayaram~Subramanya, F.~Devvrit, H.~V. Simhadri, R.~Krishnawamy, and R.~Kadekodi.
\newblock DiskANN: Fast accurate billion-point nearest neighbor search on a single node.
\newblock In {\em NeurIPS}, pages 13766--13776, 2019.

% ===== Benchmarks =====
\bibitem{annbenchmarks}
M.~Aum{\"u}ller, E.~Bernhardsson, and A.~Faithfull.
\newblock Ann-benchmarks: A benchmarking tool for approximate nearest neighbor algorithms.
\newblock {\em Information Systems}, 87:101374, 2020.

\bibitem{beir}
N.~Thakur, N.~Reimers, A.~R{\"u}ckl{\'e}, A.~Srivastava, and I.~Gurevych.
\newblock BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models.
\newblock In {\em NeurIPS Datasets and Benchmarks}, 2021.

\bibitem{bigann}
H.~J{\'e}gou, R.~Tavenard, M.~Douze, and L.~Amsaleg.
\newblock Searching in one billion vectors: Re-rank with source coding.
\newblock In {\em ICASSP}, pages 861--864, 2011.

\bibitem{sift}
D.~G. Lowe.
\newblock Distinctive image features from scale-invariant keypoints.
\newblock {\em IJCV}, 60(2):91--110, 2004.

\bibitem{tpch}
Transaction Processing Performance Council.
\newblock TPC-H benchmark specification, revision 3.0.1.
\newblock Technical report, TPC, 2021.

\bibitem{ycsb}
B.~F. Cooper, A.~Silberstein, E.~Tam, R.~Ramakrishnan, and R.~Sears.
\newblock Benchmarking cloud serving systems with YCSB.
\newblock In {\em SoCC}, pages 143--154, 2010.

% ===== Vector Database Systems =====
\bibitem{vdbsurvey2025}
J.~Pan, J.~Wang, and G.~Li.
\newblock Survey of vector database management systems.
\newblock {\em VLDB Journal}, 2024.

\bibitem{milvus}
J.~Wang, X.~Yi, R.~Guo, H.~Jin, P.~Xu, S.~Li, X.~Wang, X.~Guo, C.~Li, X.~Xu, K.~Yu, Y.~Yuan, Y.~Zou, J.~Long, Y.~Cai, Z.~Li, Z.~Zhang, Y.~Mo, J.~Gu, R.~Jiang, Y.~Wei, and C.~Xie.
\newblock Milvus: A purpose-built vector data management system.
\newblock In {\em SIGMOD}, pages 2614--2627, 2021.

\bibitem{pgvector}
A.~Katz.
\newblock pgvector: Open-source vector similarity search for Postgres.
\newblock \url{https://github.com/pgvector/pgvector}, 2021.

\bibitem{pinecone}
Pinecone Systems Inc.
\newblock Pinecone: Vector database for machine learning.
\newblock \url{https://www.pinecone.io}, 2021.

\bibitem{weaviate}
B.~van~Lith and E.~Diaz.
\newblock Weaviate: An open source vector search engine.
\newblock \url{https://weaviate.io}, 2021.

\bibitem{qdrant}
Qdrant Solutions GmbH.
\newblock Qdrant: Vector similarity search engine.
\newblock \url{https://qdrant.tech}, 2021.

\bibitem{chroma}
Chroma Inc.
\newblock Chroma: The AI-native open-source embedding database.
\newblock \url{https://www.trychroma.com}, 2022.

% ===== RAG and Embeddings =====
\bibitem{rag}
P.~Lewis, E.~Perez, A.~Piktus, F.~Petroni, V.~Karpukhin, N.~Goyal, H.~K{\"u}ttler, M.~Lewis, W.~Yih, T.~Rockt{\"a}schel, S.~Riedel, and D.~Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive NLP tasks.
\newblock In {\em NeurIPS}, pages 9459--9474, 2020.

\bibitem{dpr}
V.~Karpukhin, B.~O{\u{g}}uz, S.~Min, P.~Lewis, L.~Wu, S.~Edunov, D.~Chen, and W.~Yih.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In {\em EMNLP}, pages 6769--6781, 2020.

\bibitem{cohere}
Cohere Inc.
\newblock Cohere embeddings.
\newblock \url{https://cohere.ai/embeddings}, 2022.

\bibitem{openaiembeddings}
OpenAI.
\newblock Text embeddings.
\newblock \url{https://platform.openai.com/docs/guides/embeddings}, 2022.

\bibitem{sentence-bert}
N.~Reimers and I.~Gurevych.
\newblock Sentence-BERT: Sentence embeddings using siamese BERT-networks.
\newblock In {\em EMNLP-IJCNLP}, pages 3982--3992, 2019.

% ===== Index Maintenance and Updates =====
\bibitem{lsmtree}
P.~O'Neil, E.~Cheng, D.~Gawlick, and E.~O'Neil.
\newblock The log-structured merge-tree (LSM-tree).
\newblock {\em Acta Informatica}, 33(4):351--385, 1996.

\bibitem{btree}
R.~Bayer and E.~McCreight.
\newblock Organization and maintenance of large ordered indexes.
\newblock {\em Acta Informatica}, 1(3):173--189, 1972.

\bibitem{postgres-vacuum}
PostgreSQL Global Development Group.
\newblock PostgreSQL documentation: Routine vacuuming.
\newblock \url{https://www.postgresql.org/docs/current/routine-vacuuming.html}, 2024.

\bibitem{lucene}
Apache Software Foundation.
\newblock Apache Lucene.
\newblock \url{https://lucene.apache.org}, 2024.

\bibitem{elasticsearch}
Elastic NV.
\newblock Elasticsearch: The official distributed search and analytics engine.
\newblock \url{https://www.elastic.co/elasticsearch}, 2024.

% ===== Database Systems and Benchmarking =====
\bibitem{oltpbench}
D.~E. Difallah, A.~Pavlo, C.~Curino, and P.~Cudre-Mauroux.
\newblock OLTP-Bench: An extensible testbed for benchmarking relational databases.
\newblock {\em PVLDB}, 7(4):277--288, 2013.

\bibitem{graybenchmark}
J.~Gray.
\newblock Database and transaction processing performance handbook.
\newblock In {\em The Benchmark Handbook for Database and Transaction Systems}, 1993.

\bibitem{stonebraker2007}
M.~Stonebraker, S.~Madden, D.~J. Abadi, S.~Harizopoulos, N.~Hachem, and P.~Helland.
\newblock The end of an architectural era (it's time for a complete rewrite).
\newblock In {\em VLDB}, pages 1150--1160, 2007.

\bibitem{pavlo2017}
A.~Pavlo and M.~Aslett.
\newblock What's really new with NewSQL?
\newblock {\em SIGMOD Record}, 45(2):45--55, 2016.

% ===== Distributed Systems =====
\bibitem{raft}
D.~Ongaro and J.~Ousterhout.
\newblock In search of an understandable consensus algorithm.
\newblock In {\em USENIX ATC}, pages 305--320, 2014.

\bibitem{dynamo}
G.~DeCandia, D.~Hastorun, M.~Jampani, G.~Kakulapati, A.~Lakshman, A.~Pilchin, S.~Sivasubramanian, P.~Vosshall, and W.~Vogels.
\newblock Dynamo: Amazon's highly available key-value store.
\newblock In {\em SOSP}, pages 205--220, 2007.

% ===== Similarity Search Theory =====
\bibitem{indyk1998}
P.~Indyk and R.~Motwani.
\newblock Approximate nearest neighbors: Towards removing the curse of dimensionality.
\newblock In {\em STOC}, pages 604--613, 1998.

\bibitem{kdtree}
J.~L. Bentley.
\newblock Multidimensional binary search trees used for associative searching.
\newblock {\em Communications of the ACM}, 18(9):509--517, 1975.

\bibitem{balltree}
S.~M. Omohundro.
\newblock Five balltree construction algorithms.
\newblock Technical Report TR-89-063, ICSI, 1989.

\end{thebibliography}

%=============================================================================
% APPENDIX
%=============================================================================
\appendix

\section{Complete Cycle-by-Cycle Data}

\begin{table*}[h]
\centering
\caption{Milvus Churn Test - Complete Results}
\begin{tabular}{lccccc}
\toprule
\textbf{Cycle} & \textbf{Recall@10} & \textbf{P50 (ms)} & \textbf{P99 (ms)} & \textbf{Delete (s)} & \textbf{Insert (s)} \\
\midrule
0 & 97.84\% & 15.48 & 17.69 & -- & -- \\
1 & 97.07\% & 19.75 & 34.64 & 21.55 & 11.85 \\
2 & 96.28\% & 19.13 & 39.30 & 21.84 & 15.34 \\
3 & 95.34\% & 20.11 & 35.06 & 23.84 & 13.98 \\
4 & 94.57\% & 19.32 & 34.70 & 24.18 & 14.84 \\
5 & 93.69\% & 19.90 & 43.44 & 21.32 & 13.92 \\
6 & 92.96\% & 20.82 & 42.40 & 25.35 & 14.64 \\
7 & 91.96\% & 23.78 & 38.30 & 23.19 & 13.70 \\
8 & 91.03\% & 19.93 & 34.02 & 23.77 & 14.74 \\
9 & 90.16\% & 20.33 & 38.06 & 21.51 & 15.33 \\
10 & 89.28\% & 20.95 & 37.86 & 22.66 & 14.86 \\
\bottomrule
\end{tabular}
\end{table*}

\section{Reproducibility Checklist}

\begin{itemize}
    \item[$\checkmark$] Code available: \url{https://github.com/debu-sinha/vecops-bench}
    \item[$\checkmark$] Data available: Cohere Wikipedia (HuggingFace), SIFT1M (public)
    \item[$\checkmark$] Hardware specified: AWS EC2, 123GB RAM, 16 vCPU
    \item[$\checkmark$] Software versions pinned: See Table~\ref{tab:databases}
    \item[$\checkmark$] HNSW parameters specified: See Table~\ref{tab:hnsw}
    \item[$\checkmark$] Statistical methodology: 10,000 queries per measurement
\end{itemize}

\end{document}
