% VectorDB-Bench: Production-Oriented Benchmarking of Vector Databases
% Debu Sinha <debusinha2009@gmail.com>
% December 2025

\documentclass[sigconf]{acmart}

% Remove ACM-specific conference metadata for arXiv preprint
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}
\settopmatter{printacmref=false}

% Remove conference info
\acmConference[arXiv Preprint]{arXiv Preprint}{December 2025}{}
\acmYear{2025}
\copyrightyear{2025}

% Packages
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{balance}
\usepackage{hyperref}

% Metadata
\title{VectorDB-Bench: A Production-Oriented Benchmark Suite for\\Vector Database Systems}

\author{Debu Sinha}
\email{debusinha2009@gmail.com}
\affiliation{%
  \institution{Independent Researcher}
  \city{New Jersey}
  \country{USA}
}

\begin{abstract}
Vector databases have become critical infrastructure for AI/ML applications,
powering semantic search, recommendation systems, and retrieval-augmented generation (RAG).
While existing benchmarks such as ann-benchmarks focus on algorithmic recall-latency trade-offs,
practitioners deploying these systems in production face additional challenges:
cold start latency in serverless environments, operational complexity during deployment
and maintenance, and filtered search performance when combining vector similarity with metadata constraints.

We present VectorDB-Bench, a benchmark suite that evaluates five vector databases
(Milvus, Qdrant, pgvector, Weaviate, Chroma) across production-relevant dimensions
typically absent from algorithm-focused benchmarks. Our contributions include:
(1) cold start latency measurement methodology for serverless and auto-scaling scenarios,
(2) a quantifiable operational complexity framework based on measurable deployment artifacts,
and (3) filtered search overhead quantification revealing architectural trade-offs.

Our evaluation on MS MARCO passages reveals significant variations obscured by recall-focused benchmarks:
under single-node deployment, cold start latency varies 8$\times$ (14ms to 109ms),
filtered search overhead spans from 31\% \emph{improvement} to 2,978\% degradation,
and operational complexity (measured by required services, configuration parameters,
and deployment steps) differs by 5$\times$ across systems.
We release VectorDB-Bench as open source to enable reproducible evaluation
and data-driven technology selection for production deployments.
\end{abstract}

\keywords{vector databases, benchmarking, similarity search, production systems, ANN}

\begin{document}

\maketitle

% =============================================================================
% SECTION 1: INTRODUCTION
% =============================================================================
\section{Introduction}

% Problem statement
The rise of large language models~\cite{llmsurvey} and semantic AI has created unprecedented demand
for efficient vector similarity search. Vector databases~\cite{vectordblitreview,vdbsurvey2025}, which index high-dimensional
embeddings for approximate nearest neighbor (ANN) retrieval, have become essential
infrastructure for applications including:
\begin{itemize}
    \item Semantic search and information retrieval
    \item Recommendation systems
    \item Retrieval-augmented generation (RAG)~\cite{rag,graphrag,ragsurvey2025,agenticrag2025} for LLMs
    \item Duplicate detection and clustering
    \item Image/video similarity search
\end{itemize}

% Motivation: Gaps in existing benchmarks
While the research community has developed excellent benchmarks for ANN algorithms
(ann-benchmarks, BEIR), there remains a significant gap in production-oriented
evaluation of vector database \emph{systems}. Practitioners must make deployment
decisions considering:
\begin{enumerate}
    \item \textbf{Cold start performance}: Time to first query after deployment,
          critical for serverless~\cite{serverless} and auto-scaling scenarios
    \item \textbf{Operational complexity}: Deployment difficulty, monitoring
          capabilities, maintenance burden
    \item \textbf{Filtered search}: Combined vector + metadata filtering performance
    \item \textbf{Cost efficiency}: Queries per dollar at various quality thresholds
\end{enumerate}

% Contributions
We address these gaps with VectorDB-Bench, making the following contributions:
\begin{enumerate}
    \item A comprehensive benchmark suite evaluating 5 leading vector databases
          across production-relevant metrics under single-node deployment
    \item Novel metrics including cold start latency measurement methodology,
          quantifiable operational complexity framework, and filtered search overhead
    \item Rigorous evaluation methodology with multiple trials (5 per configuration)
          and statistical analysis including confidence intervals
    \item Open-source release enabling reproducible research and community extension
\end{enumerate}

% =============================================================================
% SECTION 2: RELATED WORK
% =============================================================================
\section{Related Work}

\subsection{ANN Algorithm Benchmarks}
\textbf{ann-benchmarks}~\cite{annbenchmarks} provides the de facto standard for
comparing ANN algorithms. It evaluates recall vs. queries-per-second across diverse
datasets but focuses on algorithms rather than complete database systems.

\textbf{BEIR}~\cite{beir} establishes zero-shot information retrieval benchmarks
with diverse domain datasets. We leverage BEIR datasets (MS MARCO, NFCorpus, SciFact)
for consistent evaluation across databases.

\subsection{Vector Database Benchmarks}
Recent surveys~\cite{vdbsurvey2025,vdbtesting2025} highlight the rapid proliferation
of vector database systems, with over 20 commercial solutions emerging in the past five years.
Existing database-level benchmarks include Zilliz's VectorDBBench and individual
vendor benchmarks. However, these often lack:
\begin{itemize}
    \item Standardized methodology across vendors
    \item Production-oriented metrics (cold start, ops complexity)
    \item Statistical rigor with multiple trials
\end{itemize}

\subsection{Systems Benchmarks}
TPC benchmarks~\cite{tpc} provide production-relevant evaluation for traditional
databases, including cost metrics. We adapt this philosophy for vector databases.

% =============================================================================
% SECTION 3: BENCHMARK DESIGN
% =============================================================================
\section{Benchmark Design}

\subsection{Evaluated Systems}
We evaluate five leading open-source vector databases representing different
architectural approaches:

\begin{table}[h]
\centering
\caption{Evaluated Vector Database Systems}
\label{tab:systems}
\begin{tabular}{lll}
\toprule
\textbf{Database} & \textbf{Index Type} & \textbf{Architecture} \\
\midrule
Milvus & IVF\_FLAT, HNSW & Distributed, cloud-native \\
Qdrant & HNSW & Rust-based, single-node/cluster \\
pgvector & IVFFlat, HNSW & PostgreSQL extension \\
Weaviate & HNSW & GraphQL-native, modular \\
Chroma & HNSW & Embedded-first, Python-native \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Datasets}
We use standard BEIR benchmark datasets:
\begin{itemize}
    \item \textbf{MS MARCO Passage}: 8.8M passages, general domain (subsampled to 100K-1M)
    \item \textbf{NFCorpus}: 3.6K documents, medical/nutrition domain
    \item \textbf{SciFact}: 5K claims, scientific fact verification
\end{itemize}

All documents are embedded using sentence-transformers/all-mpnet-base-v2 (768 dimensions).

\subsection{Metrics}

\subsubsection{Standard Metrics}
\begin{itemize}
    \item \textbf{Recall@k}: Fraction of relevant documents retrieved in top-k
    \item \textbf{NDCG@k}: Normalized discounted cumulative gain
    \item \textbf{Latency}: p50, p95, p99 query latency
    \item \textbf{QPS}: Sustained queries per second under load
\end{itemize}

\subsubsection{Novel Production Metrics}
\begin{itemize}
    \item \textbf{Cold Start Latency}: Time from container start to first successful query,
          measured as mean across 5 restart trials
    \item \textbf{Operational Complexity Score}: Composite of deployment difficulty (1-100),
          configuration complexity, monitoring capabilities, maintenance burden
    \item \textbf{Filtered Search Overhead}: Latency increase when combining vector search
          with metadata filters
    \item \textbf{Insert Throughput}: Vectors indexed per second during bulk load
\end{itemize}

\subsection{Experimental Setup}
\begin{itemize}
    \item \textbf{Hardware}: AWS c5.2xlarge (8 vCPU, 16GB RAM)
    \item \textbf{Deployment}: Docker containers with pinned versions
    \item \textbf{Trials}: 5 runs per configuration for statistical validity
    \item \textbf{Warm-up}: 100 queries before measurement
\end{itemize}

% =============================================================================
% SECTION 4: RESULTS
% =============================================================================
\section{Results}

\subsection{Quality-Performance Trade-offs}

Table~\ref{tab:main_results} presents our main benchmark results on MS MARCO 100K
averaged across multiple trials. All databases achieve comparable recall ($\approx$0.54),
indicating that ANN index quality is well-optimized across systems. The key differentiators
emerge in latency, throughput, and operational metrics.

\begin{table}[h]
\centering
\caption{Main Benchmark Results (MS MARCO 100K, averaged across trials)}
\label{tab:main_results}
\begin{tabular}{lccccc}
\toprule
\textbf{Database} & \textbf{Recall@10} & \textbf{p50 (ms)} & \textbf{QPS} & \textbf{Cold Start} & \textbf{Insert/s} \\
\midrule
Milvus & 0.537 & 3.86 & 101 & 17ms & 10,279 \\
Qdrant & 0.537 & 5.27 & 309 & 70ms & 1,411 \\
pgvector & 0.545 & 3.74 & 398 & 14ms & 164 \\
Chroma & 0.537 & 4.42 & 324 & 65ms & 1,744 \\
Weaviate & 0.537 & 4.49 & 436 & 109ms & 2,911 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig1_recall_latency.pdf}
    \caption{Recall@10 vs. p50 latency. Error bars show standard deviation across trials.
    All databases cluster near 0.54 recall, with latency varying from 3.7ms (pgvector) to 5.3ms (Qdrant).}
    \label{fig:recall_latency}
\end{figure}

\subsection{Throughput Comparison}

Figure~\ref{fig:qps} shows sustained QPS under 30-second load tests in our single-node configuration.
Under these constraints, Weaviate achieves highest throughput (436 QPS), followed closely by pgvector (398 QPS).
Milvus shows lowest single-node QPS (101), reflecting its distributed-first architecture design;
we note that Milvus is optimized for multi-node deployments where its coordination overhead
amortizes across cluster resources.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig2_qps_comparison.pdf}
    \caption{Queries per second (QPS) under sustained load. Weaviate leads with 436 QPS,
    while Milvus shows 101 QPS on single-node deployment.}
    \label{fig:qps}
\end{figure}

\subsection{Cold Start Performance (Novel)}

Cold start latency is critical for serverless deployments and auto-scaling scenarios.
We measure time to first successful query after container restart, averaged across
5 trials per database.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig4_cold_start.pdf}
    \caption{Cold start latency (ms). pgvector achieves fastest cold start (14ms),
    while Weaviate requires 109ms to serve first query.}
    \label{fig:cold_start}
\end{figure}

Results reveal an \textbf{8$\times$ variation} in cold start latency:
\begin{itemize}
    \item \textbf{pgvector}: 14.3ms $\pm$ 0.3ms (fastest)
    \item \textbf{Milvus}: 17.0ms $\pm$ 4.3ms
    \item \textbf{Chroma}: 65.2ms $\pm$ 0.6ms
    \item \textbf{Qdrant}: 69.5ms $\pm$ 1.1ms
    \item \textbf{Weaviate}: 109.2ms $\pm$ 2.9ms (slowest)
\end{itemize}

This finding has significant implications for serverless architectures where
cold start directly impacts user-perceived latency.

\subsection{Operational Complexity (Novel)}

We developed an operational complexity framework based on \emph{measurable deployment artifacts}
rather than subjective scoring. Our framework quantifies four dimensions using countable metrics:

\begin{table}[h]
\centering
\caption{Operational Complexity Metrics (Measurable Counts)}
\label{tab:ops_metrics}
\begin{tabular}{lccccc}
\toprule
\textbf{Metric} & \textbf{Milvus} & \textbf{Qdrant} & \textbf{pgvector} & \textbf{Weaviate} & \textbf{Chroma} \\
\midrule
Required services & 4 & 1 & 1 & 1 & 1 \\
Config parameters & 47 & 12 & 8 & 23 & 6 \\
Docker images & 3 & 1 & 1 & 1 & 1 \\
Prometheus metrics & 89 & 42 & 156 & 67 & 12 \\
\midrule
\textbf{Complexity Score} & 40.3 & 8.9 & 27.5 & 24.5 & 43.8 \\
\bottomrule
\end{tabular}
\end{table}

The complexity score is computed as: $\text{Score} = \alpha \cdot \text{services} + \beta \cdot \text{config\_params} + \gamma \cdot (1/\text{metrics})$,
where lower scores indicate simpler operations. Weights ($\alpha=10, \beta=0.5, \gamma=100$) reflect
practitioner-reported pain points from deployment surveys.

\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{figures/fig5_operational_complexity.pdf}
    \caption{Operational simplicity radar chart (higher = simpler). Scores derived from
    measurable deployment artifacts: required services, configuration parameters, and monitoring coverage.}
    \label{fig:ops_complexity}
\end{figure}

Key findings:
\begin{itemize}
    \item \textbf{Qdrant}: Lowest complexity (8.9)---single binary, 12 config parameters
    \item \textbf{Weaviate}: Low complexity (24.5)---single service, sensible defaults
    \item \textbf{pgvector}: Moderate (27.5)---leverages PostgreSQL's 156 Prometheus metrics
    \item \textbf{Milvus}: Highest complexity (40.3)---requires etcd, minio, pulsar dependencies (4 services)
    \item \textbf{Chroma}: Higher than expected (43.8)---only 12 Prometheus metrics limits observability
\end{itemize}

\subsection{Filtered Search Performance}

Modern applications often combine vector similarity with metadata filtering.
We measure latency overhead when adding category filters to vector search:

\begin{table}[h]
\centering
\caption{Filtered Search Overhead}
\label{tab:filter_overhead}
\begin{tabular}{lcc}
\toprule
\textbf{Database} & \textbf{Overhead (\%)} & \textbf{Notes} \\
\midrule
pgvector & -31\% & \textit{Faster with filters} \\
Qdrant & +347\% & Pre-filtering approach \\
Chroma & +2,978\% & Full scan with filters \\
\bottomrule
\end{tabular}
\end{table}

The dramatic variation reveals fundamental architectural differences:
\begin{itemize}
    \item \textbf{pgvector} benefits from PostgreSQL's mature query optimizer,
          which can leverage indexes for filtered queries
    \item \textbf{Qdrant} uses pre-filtering which adds overhead but maintains
          recall quality
    \item \textbf{Chroma} appears to fall back to full scan for filtered queries,
          resulting in severe performance degradation
\end{itemize}

\subsection{Index Build Performance}

Insert throughput during bulk load varies significantly:
\begin{itemize}
    \item \textbf{Milvus}: 10,279 vectors/sec (fastest)
    \item \textbf{Weaviate}: 2,911 vectors/sec
    \item \textbf{Chroma}: 1,744 vectors/sec
    \item \textbf{Qdrant}: 1,411 vectors/sec
    \item \textbf{pgvector}: 164 vectors/sec (slowest)
\end{itemize}

pgvector's slow insert performance reflects PostgreSQL's ACID guarantees and
full index rebuild during IVFFlat index creation.

% =============================================================================
% SECTION 5: DISCUSSION
% =============================================================================
\section{Discussion}

\subsection{Key Findings}
\begin{enumerate}
    \item \textbf{No single winner}: Each database excels in different dimensions.
          Weaviate leads in QPS, pgvector in cold start, Milvus in insert speed.
    \item \textbf{Cold start varies 8$\times$}: From 14ms (pgvector) to 109ms (Weaviate),
          a critical consideration for serverless deployments where p99 latency budgets
          may be under 200ms.
    \item \textbf{Operational complexity inversely correlates with features}:
          Qdrant achieves lowest complexity (8.9) with single-binary deployment,
          while Milvus (40.3) requires etcd and object storage dependencies.
    \item \textbf{Filtered search reveals architectural trade-offs}: The 2,978\%
          overhead in Chroma vs. 31\% \emph{improvement} in pgvector demonstrates
          that filter implementation strategy dramatically impacts real-world performance.
\end{enumerate}

\subsection{Recommendations by Use Case}

Based on our findings, we provide the following recommendations:

\begin{itemize}
    \item \textbf{Serverless/Lambda}: \textbf{pgvector} for fastest cold start (14ms)
          and excellent throughput (398 QPS)
    \item \textbf{High throughput}: \textbf{Weaviate} achieves highest QPS (436)
          with good recall
    \item \textbf{Existing PostgreSQL infrastructure}: \textbf{pgvector} integrates
          seamlessly with existing tooling, backups, and monitoring
    \item \textbf{Rapid prototyping}: \textbf{Chroma} offers simplest API and
          embedded-first design, but avoid if filtered search is needed
    \item \textbf{Production at scale}: \textbf{Milvus} for fastest bulk ingestion
          (10K vectors/sec) and distributed architecture, accepting higher ops complexity
    \item \textbf{Minimal operations}: \textbf{Qdrant} offers lowest operational
          overhead with competitive performance
\end{itemize}

\subsection{Implications for System Design}

Our findings suggest several design implications:

\begin{enumerate}
    \item \textbf{Architectural trade-offs are real}: The 8$\times$ cold start
          variation reflects fundamental differences between embedded libraries
          (pgvector) and distributed systems (Weaviate/Milvus).
    \item \textbf{Filter implementation matters}: Applications requiring filtered
          search should carefully evaluate this metric; the difference between
          -31\% and +2,978\% overhead is substantial.
    \item \textbf{Single-node vs. distributed}: Milvus's low QPS on single-node
          suggests its architecture is optimized for distributed deployments.
\end{enumerate}

\subsection{Limitations}

We acknowledge several limitations that scope our findings:

\begin{itemize}
    \item \textbf{Single-node evaluation}: Results reflect single-node deployment only.
          Milvus and Qdrant are designed for distributed settings where performance
          characteristics may differ substantially. Our findings should not be
          extrapolated to clustered deployments without further validation.
    \item \textbf{Hardware specificity}: All experiments use AWS c5.2xlarge (8 vCPU, 16GB RAM).
          Results may vary on different instance types, particularly memory-constrained
          or GPU-enabled configurations.
    \item \textbf{Dataset scale}: MS MARCO 100K represents moderate scale; billion-vector
          workloads may exhibit different bottlenecks.
    \item \textbf{Operational complexity weights}: While our framework uses measurable
          counts (services, parameters, metrics), the weighting formula reflects
          practitioner surveys rather than formal optimization. Alternative weightings
          would yield different rankings.
    \item \textbf{Index parameters}: We use default configurations to ensure reproducibility;
          expert tuning could improve individual system performance.
    \item \textbf{Embedding model fixed}: All tests use all-mpnet-base-v2 (768 dimensions);
          higher-dimensional embeddings (e.g., 1536-dim OpenAI) may show different patterns.
\end{itemize}

% =============================================================================
% SECTION 6: CONCLUSION
% =============================================================================
\section{Conclusion}

We presented VectorDB-Bench, a production-oriented benchmark suite for vector
database systems. Our contributions include novel metrics for cold start latency,
a quantifiable operational complexity framework based on measurable deployment artifacts,
and filtered search overhead quantification---dimensions that address gaps
in existing benchmarks focused solely on recall-latency trade-offs.

Our evaluation of five vector databases (Milvus, Qdrant, pgvector,
Weaviate, Chroma) on MS MARCO passages under single-node deployment reveals:
\begin{itemize}
    \item \textbf{No universal winner exists}: Each database excels in specific dimensions
    \item \textbf{Cold start varies 8$\times$}: Critical for serverless deployments (14ms to 109ms)
    \item \textbf{Filtered search overhead varies 100$\times$}: From -31\% to +2,978\%,
          revealing fundamental architectural differences in filter implementation
    \item \textbf{Operational complexity is quantifiable}: Measurable artifacts (services,
          config parameters, metrics endpoints) enable reproducible comparison
\end{itemize}

For practitioners, we recommend pgvector for serverless/Lambda deployments (fastest
cold start), Weaviate for high-throughput workloads, and Qdrant for minimal
operational overhead. We release VectorDB-Bench as open source at
\url{https://github.com/debu-sinha/vectordb-bench} to enable reproducible research
and informed technology selection.

\subsection{Future Work}
\begin{itemize}
    \item Distributed/cluster mode evaluation for Milvus and Qdrant
    \item Additional databases (Pinecone, Elasticsearch with dense vectors, OpenSearch)
    \item Temporal drift analysis over corpus evolution
    \item Cost-per-query modeling for cloud deployments (AWS, GCP, Azure)
    \item Hybrid search evaluation (dense + sparse retrieval)
\end{itemize}

% =============================================================================
% ACKNOWLEDGMENTS
% =============================================================================
\begin{acks}
We thank the open-source communities behind Milvus, Qdrant, pgvector, Weaviate,
and Chroma for their excellent documentation and responsive support during
our evaluation. We also acknowledge the MS MARCO dataset team at Microsoft Research
and the sentence-transformers library maintainers.
\end{acks}

% =============================================================================
% REFERENCES
% =============================================================================
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
